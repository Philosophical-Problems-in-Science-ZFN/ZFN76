\setcounter{secnumdepth}{1}



\title{Szablon-EN}

\begin{document}

Model Uncertainty: when modeling risk leads to pretense of knowledge





Mateusz Machaj (University of New York in Prague)



Abstract: The main purpose of the paper is to develop a~concept of \textit{model uncertainty} as opposed to the existing and well-established concept of model risk. Up to date the broad literature on probability not only developed complete probability systems, but also correctly noticed limitations of probability calculus. Despite the acknowledgement of such probability restrictions, drawbacks of modeling are often related to model risk. We present an argument here to distinguish a~feature limiting models even further: model uncertainty. The tenets of it already exist in the literature on probability, but were not properly emphasized while the idea of model risk was developed. Our plan it to start with a~broad overview of the existing knowledge about probability in order to start with fundamental principles. From them we are deriving a~new concept of model uncertainty.



Keywords: Uncertainty, Frank Knight, model risk, model uncertainty



JEL: B40, B41, D5, C00, C18



\section{Introduction}

Probability theoreticians from all disciplines have recognized for a~long time that calculation of probability has its limitations, especially when one applies it to describe existing reality, or even predict future events. Probability has wide variety of applications in various scientific fields, ranging from hard natural physical sciences, through biological sciences, to social sciences, including economics, sociology, and especially policy making. Even though, while building philosophical foundations of probability, experts virtually always recognize its shortcomings, these are often brushed aside in application to practical aspects of social sciences. There is a~notion of model risk used especially in finance regarding models used for pricing and decision making -- which is an attempt to infer the potential mistakes from wrong parameters used in the model. Yet it may suffer a~similar limiting feature as the model itself, for it has to assume something about knowing the underlying parameters (or kind of meta-parameters).



In analyzing probability two separate concepts were developed: the Knightian distinction between uncertainty and risk, which happens to parallel the Misesian distinction between case probability and class probability. Class probability (risk) is commonly associated with the traditional approach in statistics, and refers to the probability of an event based on a~long-run frequency within a~well-defined reference class. It is therefore applicable to situations where events are repeatable and strictly homogeneous, whereas case probability (uncertainty), or specific event probability, applies to unique, non-repeatable events and is based on subjective judgment rather than empirical frequency. Following this line of distinction we suggest to create a~concept of model uncertainty being parallel to already existing concept of model risk (uncertainty would here mean that we have an undetermined component which influences the outcomes and is not subjected to probability calculus). In order to arrive at it we start off with the basic principles of probability.



The first section describes the subjective perspective on the nature of probability. The second section discusses the limits of probability calculus, mostly due to Knightian uncertainty. The third section defends the notion that even under pure uncertainty there exist regularities in economies, hence economic laws. The fourth section explains how flawed probability models can lead to pretense of knowledge, thus increasing economic ignorance rather than enhancing knowledge. The fifth section discusses model risk as opposed to our notion of model uncertainty. The last section offers concluding comments.



\section{Probability as a~Solution to Ignorance}

Probability is an indispensible scientific concept. Repeated analyses of numerous events under varying circumstances do not always lead to deterministic recognition of the variables. The future of any observed system, both in the social and natural sciences, is not entirely foreseeable. Despite such lack of knowledge, we might recognize patterns of possible outcomes. Under reasonable assumptions scientists can create probability distributions of likely scenarios. The absence of full knowledge leads to partial knowledge. From this perspective probability analysis can be seen as a~partial solution to ignorance. Probability analysis produces knowledge about ignorance that helps us identify the boundaries of knowing and predicting.



Let us use the example of coin flipping by person A. Person B~is asked whether the result is going to be heads or tails. To give a~correct answer she would have to know all the relevant conditions and factors that might influence the result, including very specific circumstances of small particles and forces affecting the coin flipping. This would have to include knowledge of magnetic forces, atoms, electrons, and their relation to each other, plus of course a~perfect simulation of person A's hand throwing a~coin into the air. In other words, one would need to have a~complete model of the part of the universe inside the room to make a~correct prediction. The model would be complete, and it would be an equilibrium model of reality. Probability distributions would be worthless. Strictly speaking there would exist only two probabilities: 1 or 0. Something was sure to happen, or not to happen.



Equipped with this knowledge person B~would become like Laplace's demon, capable of giving an ultimate and complete description of the world. But human beings are not capable of creating a~complete model of the whole universe, and there are a~priori and empirical reasons to believe they will never be capable of doing so. It seems that statements about reality contain probabilities ranging from 0 and 1 because our knowledge of causal relation is necessarily deficient. Our ignorance becomes the reason for probability substituting for the unattainable ideal of full knowledge. If we knew more about the specific state of the coin, then probabilities might have been different 
%\label{ref:RNDduNU5wnOj5}(Reeves, 1988, pp.179–180).
\parencite[][pp.179–180]{reeves_theory_1988}.%




Probability limits the strictness of scientific laws. Nonetheless the recognition of limits for exact laws in physics does not justify scientific nihilism. One cannot answer with certainty whether the coin will land heads or tails up; but this does not mean one cannot say anything about the coin flipping. The role of science is to allow people to minimize their ignorance and yield information even about cases where full prediction is impossible. Even though one is not able to gather all the individual pieces of knowledge and predict the result of coin flipping, it is possible to learn something about this event (or these types of events). Observed and systematized studies on the distribution of results in such cases can increase our knowledge, although it is still partial knowledge 
%\label{ref:RNDuELnrfCUNf}(Kyburg, 1966, p.254).
\parencite[][p.254]{kyburg_probability_1966}. %
 The analysis would tell us whether something is more or less likely to follow. Assuming the analyzed event can be repeatedly observed, this ``more or less likely'' is captured more technically in the mathematical operations known as probability calculus. The principle of maximum likelihood selects preferred statistical theories 
%\label{ref:RNDvBK4kwPLQe}(Swinburne, 1971, p.328).
\parencite[][p.328]{swinburne_probability_1971}. %
 Because we recognize limits to our understanding, though, we accept the fact that a~full, Laplacean model of the universe and perfect predictability is unattainable.



Assuming a~probabilistic view of the world does not prohibit our assuming a~more general metaphysical determinism. Only one world exists, the one we are experiencing, ``and it never occurs twice in exactly the same state'' 
%\label{ref:RNDC4jJxRfgku}(Bricmont, 2002, p.4).
\parencite[][p.4]{bricmont_determinism_2002}. %
 Every event occurring in the world represents some characteristic feature of this world 
%\label{ref:RNDLxiY1xDjOx}(Fetzer, 1977, p.397).
\parencite[][p.397]{fetzer_world_1977}. %
 Yet determinism broadly understood as the rule that every effect has a~specifically related, exclusive set of causes is not the same as predictability. As Bricmont argues, just because we can lock up a~clock in a~drawer on an unattainable mountain and make its movement become unpredictable to us, does not mean that the movement itself is undetermined. Something can be unknown and unpredictable to us, but still determined by a~strict set of laws. Physics and metaphysics are not against each other in this respect. It might be the case that outside of the physical perspective Laplace's demon, or God, can describe the universe in a~more fundamental manner than probability theory does. Probability theory is merely a~specific type of theory that allows us to gather partial empirical knowledge that is better than complete ignorance. Under (the impossible ideal of) full knowledge the concept of probability would not be needed. In other words, we study probabilities because of epistemic indeterminism, not ontic indeterminism 
%\label{ref:RNDlEDVmbvBMM}(Fetzer, 1983, pp.371–372).
\parencite[][pp.371–372]{fetzer_probability_1983}.%
\footnote{Although the probabilistic view does not rule out an underlying determinism, it doesn't require it either.}



Put differently, probabilities need not really be ``out there'' in the universe. They are inherently linked to our existence in empirical reality and represent the relationship of our mind to that reality.\footnote{Bricmont 
%\label{ref:RNDO0lPY3mpu6}(2002)
\parencite*[][]{} %
 argues that even physical determinism leading to the rejection of the neo-indeterminist approach might come one day. The case of micro world and quantum laws is more complex and controversial. Probabilities can be seen as ``casual tendencies'' 
%\label{ref:RNDdAXEn8iWn8}(Shanks, 1993, p.295).
\parencite[][p.295]{shanks_time_1993}. %
 In those cases we appear to deal with irreducibly probabilistic behavior of molecules not being disrupted by additional forces 
%\label{ref:RNDsouCPNySdR}(Fetzer, 1983, p.372).
\parencite[][p.372]{fetzer_probability_1983}. %
 Yet just because the current state of knowledge does not allow us to point to any secondary factors, it does not mean that they are not there 
%\label{ref:RNDoZ8DmgRN4v}(Fetzer, 1983, p.373).
\parencite[][p.373]{fetzer_probability_1983}. %
 On the general level, Max Planck, commented, similarly to Albert Einstein, ``determinism is to be preferred over indeterminism under all circumstances, simply for the reason that determinate (\textit{bestimmte}) answer to a~question is always more valuable than an undeterminate (\textit{unbestimmte}) one'' 
%\label{ref:RNDntMzpY6NaW}(quoted in Krüger, 1986, p.281).
\parencite[quoted in][p.281]{kruger_probability_1986}.%
} Probability statements reflect the ``relation between a~body of evidence and propositions'' 
%\label{ref:RNDBDpZOOu9tg}(Moser, 1988, p.232).
\parencite[][p.232]{moser_foundations_1988}.%




\section{Limits of Probability Calculus: From Calculus to Judgment}

Ironically, the application of probability models might be risky. There is one important reason for that, which is to be found in the answer to the question, ``What is probability?'' Mathematics is not an empirical science---it is a~reflection of the mind (corresponding in some loose way to real objects). For that reason, relating mathematics to the real world is always challenging. The theory of probability, being a~mathematical science, is not different in that respect. In order to make sure that observations of real events comply with computed probability distributions, a~methodological leap is needed. If probabilities were just mathematical functions, written and worked out on computers, then they would have to be limited to mere mental gymnastics.



One of the most important probability theorists in history was the great Austrian mathematician Richard von Mises, who offered strong support for the frequency interpretation of probability.\footnote{The concept of frequency probability was of course developed much earlier than Richard von Mises. Its traces can be found in Aristotle, while, among others, Gauss, Laplace, Poisson were well aware of it (I thank anonymous referee for this point).} 
%\label{ref:RNDyHPAk8RZiV}(Modern, mainstream axiomatic foundations were built by Andrey Kolmogorov; see Howson, 1995, pp.17–18).
\parencites[mainstream axiomatic foundations were built by][]{}[see][pp.17–18]{howson_theories_1995}. %
 According to him, probabilities, understood as mathematical functions, need to be applied to certain collectives, which one can subject to repeated trials. We cannot talk of probabilities of single events, but only about classes of events constituting a~collective. Hence one cannot say there is a~90 percent probability that a~certain candidate will win the presidential elections in 2012 because it is a~one-off event. For that claim to be true one would need a~number of that type of elections, and only then could one venture probability distributions. Particular events have to be classified in terms of truly homogenous collectives (like the number of coins flipped) to be subjected to a~probability calculus.



Richard von Mises's argument was that a~collective needs to satisfy two essential conditions: relative frequencies need to tend to fixed limits and they have to be random 
%\label{ref:RND5Az0sxp6wG}(Mises, 1957, pp.28–29).
\parencite[][pp.28–29]{mises_probability_1957}. %
 If the coin is perfect, then the probability is 50 percent each for heads and tails. This does not mean, however, that for every ten throws the result will be 5 of each. What it means is that an infinite amount of throws will lead to a~distribution in which 50 percent of them will land heads up and 50 percent will land tails up. Randomness also means that if I~decide to register only every seventh flip of a~coin, probabilities would still tend to the same fixed limits---that is, for every seventh throw until infinity, probabilities would also tend to 50 percent.



Despite mathematical clarity there is an obvious problem here, since it is never possible to engage in infinite trials to identify true and ``certain'' probabilities of empirical regularities. One would have to rely instead on approximations and experiments. For Richard von Mises, a~methodological positivist, probabilities are out there in the world, existing objectively, and a~sufficient amount of controlled experiments should allow us to establish them in purely mathematical form. Thus the experiment under controlled conditions is a~bridge between pure mathematical function and reality. In this sense probability comes from experience with large elements from collectives. Estimations, however, are usually not set and fixed links between reality and mathematical formulas because they are derived from prior experiences, subjected to some unpredicted changes waiting to happen in the future (possible exception is naturally probability in fundamental physical models).



Richard's brother Ludwig offered what is probably a~better solution to this problem. (At least it is more empirical than relying on the concept of limiting frequencies.) Instead of a~criterion of randomness and limiting frequencies, where one needs infinite trials, it would be better to state that we do not know anything specific about particular elements of a~class except that they are members of that class 
%\label{ref:RNDs2p10QEehj}(Mises, 1966, p.109).
\parencite[][p.109]{mises_human_1966}. %
 Ludwig's improvement on his brother's theory, though, not only clarified probability assumptions and thereby made it better suited for empirical science, but also dramatically shifted away in a~philosophical approach. According to Ludwig, probability is not out there in the world, but comes from our reflections upon reality. Probabilities are proxies used to tame full ignorance. They are neither purely subjective, nor do they completely describe objective reality. They are not wishful thinking, and they are based on empirical evidence 
%\label{ref:RNDTjWllbRSJi}(Moser, 1988, p.233).
\parencite[][p.233]{moser_foundations_1988}.%




Because of his quasi-subjectivist approach, Ludwig von Mises noticed another form of probability, which is not applied to repeatable and homogenous events, that he called case probability. By this he referred to events, in particular related to humans, where conditions and circumstances are so specific that repeatable trials are impossible. It is questionable to use the word probability in those cases, since the class cannot be identified through experience. We cannot, for example, say what the probability is that Bill Gates will earn \$10 million next month in the same way that we may say a~coin flip lands 50 percent tails up and 50 percent heads up. The meaning is radically different. How so? Because the former event is unique and we know something more and something less about it. We cannot find an analytically useful class and yet it is not true that we know nothing distinctive about this event. Hence when we say that Trump has a~10 percent chance of winning the election, we merely express our qualitative judgment. We cannot and will not have ten identical elections, leading to ten parallel worlds, one of which would have Trump as president in it. We have only one really existing world. Moreover, the judgment of 10\% does not mean the event would not happen in this one existing world. Neither it means there was some necessary fundamental flaw in the reasoning.



The case-probability notion relates closely to Frank Knight's concept of uncertainty 
%\label{ref:RNDA60I125HTb}(Knight, 1971, pp.226–232).
\parencite[][pp.226–232]{knight_risk_1971}. %
 Knight pointed to the unknown, let us even say accidental, element in everyday life.\footnote{It seems that John Maynard Keynes 
%\label{ref:RNDarfxW2Lo4K}(1921)
\parencite*[][]{keynes_treatise_1921} %
 would also adhere to this view (though he subscribed to a~logical-relationist theory of probability). See also a~comparison between Mises and Keynes 
%\label{ref:RNDPT46rFN4ff}(Hauwe, 2007).
\parencite[][]{hauwe_john_2007}. %
 Van den Hauwe makes a~compelling case to demonstrate that both of those thinkers were subjective probability theorists. For a~comprehensible comparison between Knight and Ludwig von Mises, see 
%\label{ref:RND5lbM7j7Kef}(Hoppe, 2007).
\parencite[][]{hoppe_limits_2007}.%
} This element cannot be applied to a~probability calculus because it concerns unique events.\footnote{Although there is a~difference between stating that class probabilities do not exist and that class probabilities are not known. Economists sometimes understand ``uncertainty'' in the much narrower sense as not knowing the really existing probability distribution, or not knowing the exact position in the probability distribution.} Hence the radical conclusion that we cannot perfectly model human beings and their economic choices as elements of probability distributions. Yet this neither stops us from stating economic laws, nor from using a~probability calculus.



\section{True Uncertainty and Economic Laws}

Given true, or Knightian, uncertainty, are there truly any social universals? The impossibility of inference under uncertainty may lead one to scientific skepticism, the rejection of universal social laws, and what Lachmann 
%\label{ref:RNDOaTptNEBjN}(1976)
\parencite*[][]{lachmann_mises_1976} %
 saw a~kaleidoscopic view of the world. Every single decision, with its distinctive, unrepeatable features, reshapes existing and dynamic social reality, moving it to a~new disequilibrium. In fact very soon this disequilibrium is again disturbed by another unrepeatable and unique event. Hence an economist trying to answer the question about regularities in the economy is fooling himself. That seems to be an implication of rejecting determinism and probability determinism.



George Shackle, skeptical of the neoclassical approach, took this observation to its radical extreme: there are no strict economic laws 
%\label{ref:RNDdVdmTXNIBR}(Shackle, 1972, p.427).
\parencite[][p.427]{shackle_epistemics_1972}. %
 Uncertainty pervades everyday choices, which hence cannot be subjected to formalization. Such a~criticism refers to the deterministic approach in the form of simple marginal calculus, but also with the same strength to the probability calculus, since such calculus requires the economic world to be varying yet unchanging 
%\label{ref:RNDYrPxHniUv9}(Shackle, 1972, p.381).
\parencite[][p.381]{shackle_epistemics_1972}.%




Despite the fact that mainstream economists are not extreme Shackleans, they seem to implicitly agree with Shackle's point of view. Mathematical models are all we have and without them nothing is left. Nassim Taleb, criticizing the naïve class-probability approach, has this attitude 
%\label{ref:RNDW0K1oL5NLf}(Taleb, 2007, p.276).
\parencite[][p.276]{taleb_black_2007}. %
 One could echo here Keynes's comment on Tinbergen's works, which back in the 1930s took a~step towards greater mathematization of economics: ``I have a~feeling that Prof. Tinbergen may agree with much of my comment, but that his reaction will be to engage another ten computors [sic] and drown his sorrows in arithmetic'' 
%\label{ref:RNDfHedplYQzH}(Keynes, 1939, p.568).
\parencite[][p.568]{keynes_professor_1939}.%




Economists from the mainstream recognize this problem, no matter which school of thought they represent. Robert Lucas, brilliant pioneer of New Classical macroeconomics, recognized that under true, Knightian uncertainty neoclassical theory is not useful: ``In situations of risk, the hypothesis of rational behavior may be explainable in terms of economic theory. […] In cases of uncertainty, \textit{economic reasoning will be of} [sic] \textit{no value}`` 
%\label{ref:RNDXX155bY6t4}(Lucas, 1977, p.15, emphasis added).
\parencite[][emphasis added]{lucas_understanding_1977}. %
 Paul Samuelson, the godfather of the neoclassical synthesis and the Keynesian interpretation of business cycles, commenting on utility analysis, expressed the same opinion: ``[We should] never forget that economics can at no time become an \textit{exact} science for the reason that actual economic history is not ever what mathematicians call a~‘stationary probability distribution'. There are thus no exact simple rules to learn how to benefit from knowledge of the past. \textit{None at all}'' 
%\label{ref:RNDCodD7sRVc8}(Samuelson, 2008, pp.113–114, emphasis added).
\parencite[][emphasis added]{samuelson_asymmetric_2008}. %
 Let us notice that even though Lucas and Samuelson, both Nobel Prize winners, radically differ on macroeconomic policies and their effectiveness, in this they reach the same conclusion: Knightian uncertainty endangers their economic theories and pushes them towards Shackle's kaleidics.



The Shacklean approach to the validity of economic laws is defensible, however, only if economics exclusively relies on human beings' motives, ideas, and psychological states. If the economic subject dwells only on preference functions and expectations, then an ultrasubjectivist rejection of economic laws may seem reasonable. Naturally economics is about choices, while all choices are unique and specific events. We can grasp them individually in separation of other choices and discuss them using \textit{Verstehen}---historical understanding of specific circumstances 
%\label{ref:RNDcFKo6Atyl7}(Tucker, 1965).
\parencite[][]{tucker_max_1965}. %
 Yet economics is not about exact and particular choices, which form our historical experience. Economics is about the broad range of choices made in objective reality bounded by observable constraints. It is true that people can adopt completely randomly their subjective preferences (and also do not engage in mental gymnastics with indifference curves). But since the objects of their choices exist objectively, outside of their minds, there is at least the possibility of conditions limiting the power of humans to shape economic reality. Hence, economics can illustrate the connections between subjectively chosen ends and objectively existing means no matter what those ends are.



One example concerns expectations and budget constraints. People form expectations subjectively. No economic modeling could create a~complete description of them that could lead to full predictability of actions. If economics were only about expectations (individual human perceptions and motives), then there would be no universal economic laws, since expectations are always unique. But, as Garrison 
%\label{ref:RNDRLCa3Fxbmt}(Garrison, 2001, p.9)
\parencite[][p.9]{garrison_time_2001} %
 commented, we cannot spend our expectations. We form our expectations based on budget constraints and resource scarcity, money supplies, asset ownership and levels of debt, etc.--- the markets and institutions that limit our choices.



Since we are capable of analyzing those constraints and their effects, we are also capable of identifying economic laws, even given Knightian uncertainty. Examples (possibly debatable) include the following: ``Price controls lead to discoordination (surpluses and shortages),'' or ``The central bank cannot permanently keep the interest rates below the market level,'' or ``Increases of the money supply lead to redistribution effects.'' Economic laws do not lead to perfect forecasts. But science is not synonymous with prediction.



\section{Illusions of Certainty: Probability as a~Pretense of Knowledge}

There are certain pillars of probability calculus. As Shackle stresses, the analyzed system neither is inherently evolutionary, nor has a~``tendency to explode.'' Thus the system needs to be stable and modeled with constant features, so it is to be one particular thing 
%\label{ref:RNDWEkT6TNZ96}(Shackle, 1972, p.381).
\parencite[][p.381]{shackle_epistemics_1972}. %
 For example, regarding dice throwing, the dices need to be solid, with sufficient material strength. They cannot collapse with each and every throw because a~sufficiently large number of observations is needed for the proper modeling of probabilities 
%\label{ref:RNDIERiE0y73J}(Salmon, 1967, p.91).
\parencite[][p.91]{salmon_foundations_1967}. %
 An intermediate goal is to find out when the number becomes large enough that we can be sure that the system is not evolving and stays relatively ``stable''. An important part of this process is classification of events into classes, which is not simple 
%\label{ref:RNDe4EfpyVBJi}(Swinburne, 1971, pp.337–338).
\parencite[][pp.337–338]{swinburne_probability_1971}.%




Robert Higgs described in his article the notion of ``regime uncertainty'' 
%\label{ref:RNDFsza52V3nA}(Higgs, 1997).
\parencite[][]{higgs_regime_1997}. %
 Persistent change of economic policies increases uncertainty, causing capital and other economic resources to become idle. In a~similar manner, bad probability assignments due to government regulations can lead to illusions of certainty. They can create an impression of economic safety and push investors into malinvestments, which may end in capital consumption. Under specific conditions and assumptions probability analysis is a~solution to ignorance. But it is useful only if certain conditions are met. If those conditions do not exist it might be that probability calculus in response to government policy is not only not a~solution to ignorance, but even worse: it itself is a~source of pretense of knowledge, especially when it is based on past data, which are ``explosive'' in the Shacklean sense. Data may represent a~stable past trend, but at the same time an unsustainable, destructive trend for the future that is not visible in the past data.



Consider a~simple metaphorical Nassim Taleb's example of an owner feeding his turkey each day 
%\label{ref:RND9c8o9LePpr}(Taleb, 2007).
\parencite[][]{taleb_black_2007}. %
 Based on past behavior, the pattern of feeding hours might help us establish a~probability distribution: the likeliness that during the day the owner will show up to feed the turkey. But the probability calculus is based on important assumptions. Richard von Mises's argument was that one needs randomness and an infinite amount of trials; otherwise the calculus is just an approximation that may fail. Since that is never perfectly the case, the truer statement is that of his brother Ludwig: we do not know the individual characteristics of actions, but we know they are part of one class. Hence we start from this fact: we do not know something and yet we can accurately group and interpret historical data.



When the owner finally kills the turkey, we might conclude that we failed in our probability modeling and were surprised by a~black swan, or rather a~dead turkey. One possible conclusion is that it was a~completely unpredictable event. This would be a~comfortable explanation for the failed positivist model. Perhaps the overall prediction was fine, and we lacked sufficient experience to recognize the mistake. Hence in that case, black swans would be Shacklean demons attacking existing economic frameworks.



In the above case, calling the outcome a~black swan rests on an important assumption we made before our probability calculus: we do not know anything particular about events apart from that they are members of the same class. We can gain knowledge for feeding in the future only from past trends. But from a~different perspective, the opposite may be the case, for perhaps we do know something more, something qualitative, about the particular event than just that it is a~member of a~class. We know the purpose of the owner---why he kills the turkey. Focus on the repetitive homogenous data can lead to neglect of qualitative analysis, and create a~quasi-certainty. Repetition of that data may falsely suggest there is inherent stability in the non-evolving system.



In the late 1990's an investment fund Long-Term Capital Management (LTCM) believed itself to have found an El Dorado of business investments. It inferred from specific assumptions that it could flawlessly arbitrage government bonds. To Nobel Prize winners Robert Merton and Myron Scholes, both involved in the LTCM case, the crisis of 1997–1998 came like a~dead turkey, or black swan. We could give similar examples from the recent financial crisis. Additionally conducting an empirical analysis of past events, and constructing models built upon it with sophisticated RiskMetrics and Creditscoring programs, one would not have foreseen the Great Recession.



Such an approach is based on the assumption that we do not know more about price movements than that they are members of the same class. We do not see the potentially explosive aspect, and choose to hide important factors behind the notion of randomness. Yet there is something more to be found than randomness. From Knut Wicksell we know that the central bank cannot permanently reduce interest rates below market levels; and from Hayek (and for the mainstream, from Phelps and Friedman) we know that endogenous market forces counteract the interest-rate reduction, leading to a~recession and a~market correction\footnote{See: Wicksell 1962, Friedman 1968, Phelps 1967, Hayek 1969.}.



Similarly, in 1929 Irving Fisher famously declared that the values of stock market assets were too low. There are analyses still trying to prove him right 
%\label{ref:RNDoC4k3KYpQw}(see, for example, McGrattan and Prescott, 2003).
\parencite[see, for example,][]{mcgrattan_1929_2003}. %
 These analyses are based on the assumption that one can extrapolate future trends from historical data. Fisher's hypothesis is based on an analysis of data from 1921 to 1929 and the postulate that prices of assets, interest rates, and other significant variables were in equilibrium---that there was no tendency for the economic system to explode in the Shacklean sense.



Hence, as in our discussion of probability, one could assume that one does not know about particular variables, and construct a~seemingly viable model based on past data. It turns out, nevertheless, that one could know something about the variables, particularly about artificially low interest rates during 1921–1929, that caused an asset bubble. In that case the assets were not priced correctly; hence the extension of the trends from 1929 prices was also incorrect. If something was ``wrong'' with the data, it could mean that this ``wrongness'' could not go on forever. But this can be recognized only if we go beyond extrapolations from statistics.



The same is the case with debt and credit creation in the United States from the 1980s, which intensified after 2001. Existence of previous data is a~fact, but an extrapolation of it into the future through a~probability calculus assumes that underlying conditions are sustainable. Contrary to this we argue that we can know much more about existing cases than just that they are members of the same class. We know economic laws, which do not assume away Knightian uncertainty. Otherwise, if they do, and substitute it for objective probability calculus, they might lead to a~mere pretense of knowledge. They may become truly a~source of ignorance, rather than knowledge. For an example of how deceptive this probabilistic theory might be, consider Stiglitz's analysis of the mortgage market in 2002:



Specifically, historical data were used to create millions of potential future scenarios. […] These results regarding the risk-based capital standard are striking: They suggest that on the basis of historical experience, the risk to the government from a~potential default on GSE debt is \textit{effectively zero}. […] The first potential shortcoming is that the risk-based capital standard, while based on a~hypothetical economic shock significantly more severe than anything that the economy has actually experienced over the past forty years, may fail to reflect the probability of another Great Depression-like scenario. Fundamentally, the extremely rare events located in the tail of a~distribution are often quite difficult to analyze accurately. Interestingly, however, the Office of Management and Budget tested Fannie Mae's and Freddie Mac's capital adequacy in the early 1990s by subjecting their business activities to a~ten-year stress test that simulated the financial and economic conditions of the Great Depression. The test showed that if a~Depression lasted ten years, given 1990 levels of capital, both Fannie Mae and Freddie Mac had sufficient capital to survive. This result led OMB to conclude that in the event of a~severe nationwide economic downturn, the probability of either Fannie Mae or Freddie Mac defaulting would be ``\textit{close to zero}.'' 
%\label{ref:RNDoemulejcPj}(Stiglitz, Orszag and Orszag, 2002, p.5, emphasis added)
\parencite[][emphasis added]{stiglitz_implications_2002}%




In the foreword to this paper, Arne Christenson, senior vice president for regulatory policy of Fannie Mae, commented that probability of a~default was ``effectively zero.'' His approach was based on econometric analysis and simulations, which in their nature are the same as the equations used in the banking system, based on Basel regulatory rules. Under those regulations banks are supposed to measure risk and assign it to particular assets in order to protect themselves by raising a~sufficient amount of capital. The cornerstone of the problem lies in the fact that there is no universal probability calculus that one could apply to an economy subjected to credit expansions;\footnote{On a~more general level, take the case of the so called ``operational risk'': the risk that the employer will steal the funds from the institution. It begs Richard von Mises's question, how could such an incidental and unique event be assigned with an objective probability measure?} hence those ``probabilities'' and ``risks'' should be called what they truly are: subjective probabilities or judgments based on historical data. Most American banks before the crisis of 2008 achieved relatively high capital ratios, and yet they severely suffered from the crisis. Even though, on paper, risk was properly measured and secured by capital, the crisis strongly hit financial institutions. Apparently, regulations reduced uncertainty. In reality, uncertainty was hidden under the veil of pretense of knowledge and this led to capital consumption and property misusage.\footnote{See on this Jabłecki and Machaj 2009.}



\section{Model Risk versus Model Uncertainty}

One way to curtail uncertainty about grouping economic events into class probabilities is to subject the act of calculating itself to calculus. This is called model risk. To use again the coin-flipping example, it is possible that the coin may not be perfect---and the model builder may be wrong in assuming some probability distribution. The model-risk approach subjects model building itself to probability calculus to do away with the model's uncertainty. As we can see, this shifts the problem to another level, for now the act of model building itself has to be a~member of some homogenous class, and we would need repeatable experience. Sufficiently large numbers would produce statistics. Hence for example we could speculate that a~model of 50 percent probability of heads has a~90 percent chance of working, but other models, though less likely to succeed, are still possibly correct.\footnote{After the 2008 BASEL regulators introduced the necessity for capital reserves associated with ``model risk'' 
%\label{ref:RNDdETPOr7vSk}(Alexander and Sarabia, 2012, p.1295).
\parencite[][p.1295]{alexander_quantile_2012}.%
}



The above considerations are particularly relevant for financial markets. Derivatives markets depend heavily on theoretical models traders use in their transactions 
%\label{ref:RNDJYAW6LfBEh}(Green and Figlewski, 1999, p.1466).
\parencite[][p.1466]{green_market_1999}. %
 There are problems in estimating and specifying various types of risks. Since the risks are not easily verified, assessing how reliable the model is can be problematic. In proceeding to model model risk we could compare the model used to some imaginary ``correct'' model, or take one model and compare it to a~bunch of other models as though they were of one class 
%\label{ref:RNDlCi2uRJnbj}(Kerkhof, Melenberg and Schumacher, 2010, p.268).
\parencite[][p.268]{kerkhof_model_2010}. %
 Under the assumption that the ``correct model'' is still an idealized one and works as virtual reality with well-established parameters and structure\footnote{I~thank an anonymous referee for this point.}.



This comparison can help shield the company from risk, especially if it leads the company to create an additional capital reserve. But does this actually do away with uncertainty to such an extent as to potentially secure the economy from macroeconomic crises? It all depends on the underlying theory and how we treat the explosive aspect of credit expansions. Part of the problem with model risk comes with identifying the distribution in the tails---assigning probabilities to ``random'' and less predictable events (see Nassim Taleb)---or the existence of markets that are not ``perfect'' such that any arbitrage may take place. Another issue relates to the positivist reliance on observable data. Not all input parameters are observable 
%\label{ref:RNDgqQAZcAvNn}(Green and Figlewski, 1999, p.1467):
\parencite[][p.1467]{green_market_1999}:%




In particular, even if one has a~correctly specified model, using it requires knowledge of the volatility of the underlying asset over the entire lifetime of the contract. This creates a~formidable forecasting problem, for which neither the {\textquotedbl}best{\textquotedbl} estimation procedure nor the model risk characteristics of the resulting theoretical option values are known.



Futhermore according to Green and Figlewski three known sources of model risk are (1) tail distribution, (2) with non-observable input parameters (and hence wrong estimations resulting from the impossibility to have future knowledge of all asset changes) and also (3) non-continuous markets (and hence arbitrage is not working infinitely long to equilibrate). Yet their ``quantitative impact is not known.'' Therefore as Alexander and Sarbia 
%\label{ref:RNDlH8ND5Fwkx}(2012, pp.1295–1296)
\parencite*[][pp.1295–1296]{alexander_quantile_2012} %
 comment:



Outside of a~simulation environment, the concept of a~``true'' model against which one might assess model risk is meaningless. All we have is some observable data and our beliefs about the conditional and/or unconditional distribution of the random variable in question. As a~result, model risk can only be assessed relative to some benchmark model, which itself is a~matter for subjective choice.…



[O]utside of an experimental or simulation environment, we never know the ``true'' model for sure. In practice, all we can observe are realizations of the data generation processes for the random variables in our model. It is futile to propose the existence of a~unique and measurable ``true'' process because such an exercise is beyond our realm of knowledge. (emphasis added)



As Alexander and Sarabia discuss further, parameter uncertainty and model choice condition model risk; therefore model risk cannot assume true uncertainty away.\footnote{Especially in the case of exotic instruments 
%\label{ref:RNDOe9woNEgaw}(Hull and Suo, 2002, p.298).
\parencite[][p.298]{hull_methodology_2002}.%
} For those reasons the term ``model risk'' does not adequately apply to the nature of the problem inferred. The more proper name would be \textit{model uncertainty}, which would be a~qualitative margin of stating: there may be \textit{something} wrong with the model; \textit{something} that cannot be quantitatively expressed and compared to an imaginary perfect scenario.



Although data analyses can no doubt be helpful in risk assessments, exclusive focus on past data is not sufficient for good model choice. Same applies to a~meta approach of generating model risk with assigned probabilities of being successful with the chosen models. What we need is a~proper economic theory that helps to go beyond visible data, and allows us to notice the Shacklean epistemic probability, Misesian case probability, or Knightian uncertainty associated with something outside of empirically witnessed computation numbers.



\section{Conclusion}

The theory of probability is a~significant scientific tool that should not be underemphasized. Its utility, however, is based on correct recognition of its limits. Only then will probability analysis increase our knowledge and capability of prediction. If, on the other hand, we apply probability calculus to instances where it should not be, especially as we may in economics and regulatory policies, we can get erroneous results and cause even more ignorance than it aspires to reduce. Probability theorist are well aware of model limitations, therefore they try to develop notions of ``model risk'', which is in a~way extension of a~traditional approach to risk based on the notion of probability distributions.



Yet as we have seen, it cannot fully solve the problem of true Knightian uncertainty, reflecting the challenges of Misesian case probability. Therefore even if model risk may be helpful in tackling parametric mistakes, there still remains a~possibility that models do not capture some things that cannot be modelled. Literature of probability concepts created a~distinction between risk and uncertainty, hence it would be appropriate to use the term ``model uncertainty'' parallel to model risk, since some aspects cannot be parametrized under the notion of probability measurements. In other words, there is some non-measurable element in choosing correct and incorrect economic models and this also applies to meta considerations of inter-model comparisons. An element of ``true model uncertainty'', which is not subjected to similar calculus as model risk is. The main benefit of such an approach is to extend economic interpretations of true uncertainty and apply them also to broader model considerations.



\section{References}

Alexander, C. and Sarabia, J.M., 2012. Quantile Uncertainty and Value-at-Risk Model Risk. \textit{Risk Analysis}, [online] 32(8), pp.1293–1308. https://doi.org/10.1111/j.1539-6924.2012.01824.x.



Bricmont, J., 2002. \textit{Determinism, Chaos and Quantum Mechanics.} Available at: {\textless}https://www.dogma.lu/txt/JB-Determinism.pdf{\textgreater} [Accessed 8 October 2024].



Fetzer, J.H., 1977. A~world of dispositions. \textit{Synthese}, [online] 34(4), pp.397–421. https://doi.org/10.1007/BF00485648.



Fetzer, J.H., 1983. Probability and objectivity in deterministic and indeterministic situations. \textit{Synthese}, [online] 57(3), pp.367–386. https://doi.org/10.1007/BF01064703.



Garrison, R.W., 2001. \textit{Time and Money: The Macroeconomics of Capital Structure}. Foundations of the market economy. London; New York: Routledge.



Green, T.C. and Figlewski, S., 1999. Market Risk and Model Risk for a~Financial Institution Writing Options. \textit{The Journal of Finance}, [online] 54(4), pp.1465–1499. https://doi.org/10.1111/0022-1082.00152.



Hauwe, L. van den, 2007. \textit{John Maynard Keynes and Ludwig von Mises on Probability}. MPRA Paper, No. 6965. [online] MPRA - Munich Personal RePEc Archive. pp.1–46. Available at: {\textless}https://publicacion-digital.procesosdemercado.com/index.php/inicio/article/view/315{\textgreater} [Accessed 8 October 2024].



Higgs, R., 1997. Regime Uncertainty: Why the Great Depression Lasted So Long and Why Prosperity Resumed after the War. \textit{The Independent Review}, [online] 1(4), pp.561–590. Available at: {\textless}https://www.jstor.org/stable/24560785{\textgreater} [Accessed 8 October 2024].



Hoppe, H.-H., 2007. The Limits of Numerical Probability: Frank H. Knight and Ludwig von Mises and The Frequency Interpretation. \textit{The Quarterly Journal of Austrian Economics}, [online] 10(1), pp.1–20. https://doi.org/10.1007/s12113-007-9005-3.



Howson, C., 1995. Theories of Probability. \textit{The British Journal for the Philosophy of Science}, [online] 46(1), pp.1–32. https://doi.org/10.1093/bjps/46.1.1.



Hull, J. and Suo, W., 2002. A~Methodology for Assessing Model Risk and Its Application to the Implied Volatility Function Model. \textit{The Journal of Financial and Quantitative Analysis}, [online] 37(2), p.297. https://doi.org/10.2307/3595007.



Kerkhof, J., Melenberg, B. and Schumacher, H., 2010. Model risk and capital reserves. \textit{Journal of Banking \& Finance}, [online] 34(1), pp.267–279. https://doi.org/10.1016/j.jbankfin.2009.07.025.



Keynes, J.M., 1921. \textit{A~Treatise on Probability}. [online] London: Macmillan and Co. Available at: {\textless}https://catalog.hathitrust.org/api/volumes/oclc/182544.html{\textgreater} [Accessed 8 October 2024].



Keynes, J.M., 1939. Professor Tinbergen's method. \textit{The Economic Journal}, [online] 49(195), pp.558–577. https://doi.org/10.1093/ej/49.195.558.



Knight, F.H., 1971. \textit{Risk, Uncertainty and Profit}. Chicago: University of Chicago Press.



Krüger, L., 1986. Probability as a~Theoretical Concept in Physics. \textit{PSA: Proceedings of the Biennial Meeting of the Philosophy of Science Association}, [online] 1986(2), pp.273–287. https://doi.org/10.1086/psaprocbienmeetp.1986.2.192806.



Kyburg, H.E., 1966. Probability and Decision. \textit{Philosophy of Science}, [online] 33(3), pp.250–261. https://doi.org/10.1086/288097.



Lachmann, L.M., 1976. From Mises to Shackle: An Essay on Austrian Economics and the Kaleidic Society. \textit{Journal of Economic Literature}, [online] 14(1), pp.54–62. Available at: {\textless}https://www.jstor.org/stable/2722803{\textgreater} [Accessed 8 October 2024].



Lucas, R.E., 1977. Understanding business cycles. \textit{Carnegie-Rochester Conference Series on Public Policy}, [online] 5, pp.7–29. https://doi.org/10.1016/0167-2231(77)90002-1.



McGrattan, E.R. and Prescott, E.C., 2003. \textit{The 1929 Stock Market: Irving Fisher Was Right}. Research Department Staff Report No. 294. [online] Minneapolis: Federal Reserve Bank of Minneapolis. Available at: {\textless}https://core.ac.uk/download/pdf/6717612.pdf{\textgreater} [Accessed 8 October 2024].



Mises, L. von, 1966. \textit{Human Action: A~Treatise on Economics}. 3., rev. ed ed. Chicago: Regnery.



Mises, R. von, 1957. \textit{Probability, Statistics and Truth}. 2d rev. English ed. ed. New York: Macmillan Co.



Moser, P.K., 1988. The foundations of epistemological probability. \textit{Erkenntnis}, [online] 28(2), pp.231–251. https://doi.org/10.1007/BF00166444.



Reeves, T.V., 1988. A~Theory of Probability. \textit{The British Journal for the Philosophy of Science}, [online] 39(2), pp.161–182. https://doi.org/10.1093/bjps/39.2.161.



Salmon, W.C., 1967. \textit{The Foundations of Scientific Inference}. [online] Pittsburgh: University of Pittsburgh Press. https://doi.org/10.2307/j.ctt5hjqm2.



Samuelson, P.A., 2008. Asymmetric or symmetric time preference and discounting in many facets of economic theory: A~miscellany. \textit{Journal of Risk and Uncertainty}, [online] 37(2–3), pp.107–114. https://doi.org/10.1007/s11166-008-9047-8.



Shackle, G.L.S., 1972. \textit{Epistemics \& Economics: A~Critique of Economic Doctrines}. Cambridge: Cambrdige University Press.



Shanks, N., 1993. Time and the propensity interpretation of probability. \textit{Journal for General Philosophy of Science}, [online] 24(2), pp.293–302. https://doi.org/10.1007/BF00764391.



Stiglitz, J.E., Orszag, J.M. and Orszag, P.R., 2002. Implications of the New Fannie Mae and Freddie Mac Risk-based Capital Standard. \textit{Fannie Mae Papers}, 1(2), pp.1–10.



Swinburne, R.G., 1971. The Probability of Particular Events. \textit{Philosophy of Science}, [online] 38(3), pp.327–343. https://doi.org/10.1086/288374.



Taleb, N.N., 2007. \textit{The Black Swan: The Impact of the Highly Improbable}. New York: Random House.



Tucker, W.T., 1965. Max Weber's \textit{Verstehen}. \textit{The Sociological Quarterly}, 6(2), pp.157–164. https://doi.org/10.1111/j.1533-8525.1965.tb01649.x.

\end{document}

